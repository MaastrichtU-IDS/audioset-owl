{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the AudioSet ontology\n",
    "\n",
    "Using [OwlReady2](https://owlready2.readthedocs.io/en/latest/index.html) package. Ontology documentation published at https://maastrichtu-ids.github.io/audioset-owl\n",
    "\n",
    "First define the Notebook parameters for [papermill](https://papermill.readthedocs.io/en/latest/usage-parameterize.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Papermill parameters. Do not delete this cell.\n",
    "output_format = 'rdfxml'\n",
    "audioset_ontology_uri = 'https://w3id.org/audioset'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the library and define the local `ontologies` folder. If an URL is given, first searches for a local copy of the OWL file and, if not found, tries to download it from the Internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* Owlready2 * Warning: optimized Cython parser module 'owlready2_optimized' is not available, defaulting to slower Python implementation\n"
     ]
    }
   ],
   "source": [
    "from owlready2 import *\n",
    "import types\n",
    "\n",
    "if output_format == 'ntriples':\n",
    "    output_extension = 'nt'\n",
    "else:\n",
    "    output_extension = 'rdf'\n",
    "\n",
    "global audioset_onto \n",
    "global audioset_curated_hash\n",
    "onto_path.append(\"/notebooks/ontologies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and load ontologies\n",
    "\n",
    "Create the AudioSet ontology and load the Pizza ontology from the Internet (for example purpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "audioset_onto = get_ontology(audioset_ontology_uri)\n",
    "\n",
    "pizza_onto = get_ontology(\"http://www.lesfleursdunormal.fr/static/_downloads/pizza_onto.owl\").load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create AudioSet OWL ontology from the JSON\n",
    "\n",
    "* Get [AudioSet ontology JSON from GitHub](https://github.com/audioset/ontology)\n",
    "    * [AudioSet Top classes](https://research.google.com/audioset/ontology/index.html): Human sounds, Animal, Music, Sounds of things, Natural sounds, source-ambiguous things, \"Channel, environment and background\"\n",
    "* Add classes respecting hierarchy provided in the JSON through the `child_ids` field\n",
    "\n",
    "See [OwlReady2 documentation](https://owlready2.readthedocs.io/en/latest/index.html) for:\n",
    "* [Dynamic Classes](https://owlready2.readthedocs.io/en/latest/class.html#creating-classes-dynamically)\n",
    "* [Add annotations to a Class](https://owlready2.readthedocs.io/en/latest/annotations.html?highlight=comment#adding-an-annotation): `comment`, `isDefinedBy`, `label`, `seeAlso`, `backwardCompatibleWith`, `deprecated`, `incompatibleWith`, `priorVersion`, `versionInfo`\n",
    "* [Properties](https://owlready2.readthedocs.io/en/latest/properties.html)\n",
    "\n",
    "Note: classes with multiple parents are properly defined, see `ChirpTweet` or the graph visualization as example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests, json\n",
    "audioset_json = json.loads(requests.get(\"https://raw.githubusercontent.com/audioset/ontology/master/ontology.json\").text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_owl_class(class_json, parent_class):\n",
    "    \"\"\"Recursively generates OWL classes and instances, original hierarchy respected using child_ids.\"\"\"\n",
    "    with audioset_onto:\n",
    "        NewClass = types.new_class(class_json['uri_id'], (parent_class,))\n",
    "        NewClass.label = locstr(class_json['name'], lang = \"en\")\n",
    "        NewClass.comment = locstr(class_json['description'], lang = \"en\")\n",
    "        NewClass.comment = class_json['id']\n",
    "        if class_json['citation_uri']:\n",
    "            NewClass.comment = class_json['citation_uri']\n",
    "        if class_json['positive_examples']:\n",
    "            # Generate instances\n",
    "            for youtube_example in class_json['positive_examples']:\n",
    "                NewClass(comment = 'https://' + youtube_example)\n",
    "    for child in class_json['child_ids']:\n",
    "        generate_owl_class(audioset_curated_hash[child], NewClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes in the original AudioSet JSON: 632\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "# Create a hash using google audioset ID as key\n",
    "audioset_curated_hash = {}\n",
    "for row in audioset_json:\n",
    "    # Generate the ID that will be used for the ontology URI\n",
    "    uri_id = row['name'].replace(',', '').replace(')', '').replace('(', '').replace('.', '').replace(\"'\", '').replace(\";\", '')\n",
    "    uri_id = uri_id.title().replace(' ', '').replace('-', '')\n",
    "    audioset_curated_hash[row['id']] = row\n",
    "    audioset_curated_hash[row['id']]['uri_id'] = uri_id\n",
    "    c += 1\n",
    "print('Number of classes in the original AudioSet JSON: ' + str(c))\n",
    "    \n",
    "# Recursively generates classes starting from AudioSet top classes\n",
    "audioset_top_classes = ['/m/0dgw9r', '/m/0jbk', '/m/04rlf', '/t/dd00041', '/m/059j3w', '/t/dd00098', '/t/dd00123']\n",
    "for top_class in audioset_top_classes:\n",
    "    generate_owl_class(audioset_curated_hash[top_class], Thing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example to generate properties with domain and ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with audioset_onto:\n",
    "#     class Accent(Thing):\n",
    "#         pass\n",
    "#     class has_accent(ObjectProperty):\n",
    "#         domain    = [HumanVoice]\n",
    "#         range     = [Accent]\n",
    "#     class description(ObjectProperty):\n",
    "#         range     = [str]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add metadata to the ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "audioset_onto.metadata.comment.append(\"OWL Ontology for the AudioSet ontology from Google defined in JSON.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the ontology file\n",
    "\n",
    "Ontology files saved in the `ontologies` folder. \n",
    "\n",
    "2 formats available, defined in the papermill parameters (at the start of the notebook or in the `papermill-config.json` file):\n",
    "* `rdfxml`\n",
    "* `ntriples`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "audioset_onto.save(file = \"ontologies/audioset.\" + output_extension, format = output_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the ontology\n",
    "\n",
    "**With OwlReady2**, e.g. list an ontology classes and properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://w3id.org/audioset#HumanVoice\n",
      "[]\n",
      "audioset.insidesmallroom1\n",
      "audioset.insidesmallroom2\n",
      "audioset.insidesmallroom3\n",
      "audioset.insidesmallroom4\n",
      "audioset.insidesmallroom5\n",
      "audioset.insidesmallroom6\n",
      "audioset.insidesmallroom7\n",
      "audioset.insidesmallroom8\n"
     ]
    }
   ],
   "source": [
    "# Get a class IRI:\n",
    "print(audioset_onto.HumanVoice.iri)\n",
    "# List all 682 classes:\n",
    "#print(list(audioset_onto.classes()))\n",
    "# List object properties:\n",
    "print(list(audioset_onto.object_properties()))\n",
    "# List a class instances:\n",
    "for i in audioset_onto.InsideSmallRoom.instances(): print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Ontospy to analyze the ontology\n",
    "\n",
    "Load the ontology file with `ontospy`, then:\n",
    "* print top classes and the class tree\n",
    "* print instances of a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mReading: <ontologies/audioset.rdf>\u001b[0m\n",
      ".. trying rdf serialization: <xml>\u001b[0m\n",
      "\u001b[1m..... success!\u001b[0m\n",
      "\u001b[37m----------\n",
      "Loaded 12223 triples.\n",
      "----------\u001b[0m\n",
      "\u001b[32mRDF sources loaded successfully: 1 of 1.\u001b[0m\n",
      "\u001b[37m..... 'ontologies/audioset.rdf'\u001b[0m\n",
      "\u001b[37m----------\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mScanning entities...\u001b[0m\n",
      "\u001b[2m----------\u001b[0m\n",
      "\u001b[2mOntologies.........: 1\u001b[0m\n",
      "\u001b[2mClasses............: 632\u001b[0m\n",
      "\u001b[2mProperties.........: 0\u001b[0m\n",
      "\u001b[2m..annotation.......: 0\u001b[0m\n",
      "\u001b[2m..datatype.........: 0\u001b[0m\n",
      "\u001b[2m..object...........: 0\u001b[0m\n",
      "\u001b[2mConcepts (SKOS)....: 0\u001b[0m\n",
      "\u001b[2mShapes (SHACL).....: 0\u001b[0m\n",
      "\u001b[2m----------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import ontospy\n",
    "audioset_spy = ontospy.Ontospy(\"ontologies/audioset.rdf\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Class *https://w3id.org/audioset#Animal*>,\n",
       " <Class *https://w3id.org/audioset#ChannelEnvironmentAndBackground*>,\n",
       " <Class *https://w3id.org/audioset#HumanSounds*>,\n",
       " <Class *https://w3id.org/audioset#Music*>,\n",
       " <Class *https://w3id.org/audioset#NaturalSounds*>,\n",
       " <Class *https://w3id.org/audioset#SoundsOfThings*>,\n",
       " <Class *https://w3id.org/audioset#SourceAmbiguousSounds*>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# audioset_spy.printClassTree()\n",
    "audioset_spy.toplayer_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://w3id.org/audioset#sigh1 audioset:sigh1\n",
      "\u001b[31mhttps://w3id.org/audioset#sigh1\u001b[0m\u001b[0m\n",
      "\u001b[30m=> http://www.w3.org/2000/01/rdf-schema#comment\u001b[0m\n",
      "\u001b[2m.... https://youtu.be/XOphuM8ZUhM?start=560&end=570\u001b[39m\u001b[0m\n",
      "\u001b[30m=> http://www.w3.org/1999/02/22-rdf-syntax-ns#type\u001b[0m\n",
      "\u001b[2m.... https://w3id.org/audioset#Sigh\u001b[39m\u001b[0m\n",
      "\u001b[30m=> http://www.w3.org/1999/02/22-rdf-syntax-ns#type\u001b[0m\n",
      "\u001b[2m.... http://www.w3.org/2002/07/owl#NamedIndividual\u001b[39m\u001b[0m\n",
      "\n",
      "https://w3id.org/audioset#sigh2 audioset:sigh2\n",
      "\u001b[31mhttps://w3id.org/audioset#sigh2\u001b[0m\u001b[0m\n",
      "\u001b[30m=> http://www.w3.org/1999/02/22-rdf-syntax-ns#type\u001b[0m\n",
      "\u001b[2m.... https://w3id.org/audioset#Sigh\u001b[39m\u001b[0m\n",
      "\u001b[30m=> http://www.w3.org/2000/01/rdf-schema#comment\u001b[0m\n",
      "\u001b[2m.... https://youtu.be/giY25pWyJxM?start=140&end=150\u001b[39m\u001b[0m\n",
      "\u001b[30m=> http://www.w3.org/1999/02/22-rdf-syntax-ns#type\u001b[0m\n",
      "\u001b[2m.... http://www.w3.org/2002/07/owl#NamedIndividual\u001b[39m\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print instances of Sigh class\n",
    "audioset_spy.get_class('Sigh')[0]\n",
    "for instance in audioset_spy.get_class('Sigh')[0].instances:\n",
    "        print(instance.uri, instance.qname)\n",
    "        instance.printTriples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize with Ontospy docs\n",
    "\n",
    "Experimental, it is recommended to generate the documentation from the commandline (cf. `README.md` file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ontospy.ontodocs.viz.viz_html_single import *\n",
    "\n",
    "# v = HTMLVisualizer(audioset_spy) # => instantiate the visualization object\n",
    "# v.build(\"/notebooks/docs\") # => render visualization. You can pass an 'output_path' parameter too\n",
    "# v.preview() # => open in browser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize with WebVOWL\n",
    "\n",
    "Use the URL to the ontology file:\n",
    "\n",
    "[http://www.visualdataweb.de/webvowl/#iri=https://raw.githubusercontent.com/MaastrichtU-IDS/audioset-owl/master/ontologies/audioset.rdf](http://www.visualdataweb.de/webvowl/#iri=https://raw.githubusercontent.com/MaastrichtU-IDS/audioset-owl/master/ontologies/audioset.rdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize as graph using networkx\n",
    "\n",
    "Use `rdflib` and `networkx` to load the data in the graph and display it (not working with the ontology size, to be improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import rdflib\n",
    "# from rdflib.extras.external_graph_libs import rdflib_to_networkx_multidigraph\n",
    "# import networkx as nx\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# g = rdflib.Graph()\n",
    "# result = g.parse('ontologies/audioset.rdf', format='xml')\n",
    "\n",
    "# G = rdflib_to_networkx_multidigraph(result)\n",
    "\n",
    "# # Plot Networkx instance of RDF Graph\n",
    "# pos = nx.spring_layout(G, scale=3)\n",
    "# edge_labels = nx.get_edge_attributes(G, 'r')\n",
    "# nx.draw_networkx_edge_labels(G, pos, labels=edge_labels)\n",
    "# nx.draw(G, with_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
